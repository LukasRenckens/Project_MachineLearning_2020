%% Initialize
clear ; close all;

%% Read data
data_table = readtable('cars_custom.txt');

% price_usd | Odometer_value | year_produced | engine_capacity | nr_of_photos | up_counter | duration_listed
data_array = table2array(data_table(:, [15 5 6 10 17 18 29]));   

% Add numbering column
% numbers_array = [1:size(data_array)]';
% data_array = cat(2, numbers_array, data_array); 

%% Process data
%data_array = data_array(randperm(size(data_array,1)),:); % Randomize order

training = data_array([1:23113], :);               % Trainging set 60%
test = data_array([23114:30817], :);               % Test set 20%
cv = data_array([30817:size(data_array)], :);      % Cross validation set 20%

training = training(1:100,:);                % Take first few
cv = cv(1:100,:);

y = training(:,1);
X = training(:,3);                       % Year produced  
ycv = cv(:,1);
Xcv = cv(:,3);

m = length(y);                           % number of training examples
[X mu sigma] = featureNormalize(X);      % Normalize every feature ~ -3<X<+3  

X = [ones(m, 1), X];                     % Add a column of ones to x
Xcv = [ones(size(Xcv, 1), 1), Xcv];

theta = zeros(2, 1);                     % initialize fitting parameters

%% Plot data
% plotData(training(:,2), training(:,1), 'Odometer value');
% plotData(training(:,3), training(:,1), 'Production year');
% plotData(training(:,4), training(:,1), 'Engine capacity');
% plotData(training(:,5), training(:,1), 'Number of photos');
% plotData(training(:,6), training(:,1), 'Up counter');
% plotData(training(:,7), training(:,1), 'Duration listed');

plotData(X(:,2), y, 'Production year');

%% Cost and Gradient descent
% Some gradient descent settings
iterations = 1500;
alpha = 0.1;
lambda = 0;

fprintf('\nRunning Gradient Descent ...\n')
% run gradient descent
%theta = gradientDescent(X, y, theta, alpha, iterations, lambda);
theta = trainLinearReg(X, y, lambda);

% print theta to screen
fprintf('Theta found by gradient descent:\n');
fprintf('%f\n', theta);

J = computeCost(X, y, theta, lambda);
fprintf('Cost computed = %f\n', J);

% Plot the linear fit
hold on; % keep previous plot visible
plot(X(:,2), X*theta, '-b')
legend('Training data', 'Linear regression')
hold off % don't overlay any more plots on this figure

%% Learning curve
[error_train, error_cv] = learningCurve(X, y, Xcv, ycv, lambda);

figure;
plot(1:size(X, 1), error_train);
title('Train')

figure;
plot(1:size(X, 1), error_cv);
title('Cross validation');

%title('Learning curve for linear regression')
%legend('Train', 'Cross Validation')
xlabel('Number of training examples')
ylabel('Error')

% fprintf('# Training Examples\tTrain Error\tCross Validation Error\n');
% for i = 1:m
%     fprintf('  \t%d\t\t%f\t%f\n', i, error_train(i), error_val(i));
% end


%% Odometer value
% 
% y = price_array;
% X = odo_array;
% m = length(y);                                           % number of training examples
% [X mu sigma] = featureNormalize(X);                      % Normalize every feature ~ -3<X<+3    
% X = [ones(m, 1), X];                                     % Add a column of ones to x
% theta = zeros(2, 1);                                     % initialize fitting parameters
%
% %% Plot data
% 
% plotData(X(:,2), y, 'Odometer value');
% 
% %% Cost and Gradient descent
% 
% % Some gradient descent settings
% iterations = 1500;
% alpha = 0.1;
% 
% fprintf('\nRunning Gradient Descent ...\n')
% % run gradient descent
% %theta = gradientDescent(X, y, theta, alpha, iterations, lambda);
% theta = trainLinearReg(X, y, lambda);
% 
% % print theta to screen
% fprintf('Theta found by gradient descent:\n');
% fprintf('%f\n', theta);
% 
% J = computeCost(X, y, theta, lambda);
% fprintf('Cost computed = %f\n', J);
% 
% % Plot the linear fit
% hold on; % keep previous plot visible
% plot(X(:,2), X*theta, '-b')
% legend('Training data', 'Linear regression')
% hold off % don't overlay any more plots on this figure
% 
% 


